{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl2ErmA4DcoN"
   },
   "source": [
    "## Table of Contents:\n",
    "Sentiment Analysis of Movie Reviews:\n",
    "1. EDA\n",
    "2. Data Preprocessing\n",
    "3. Feature Extraction\n",
    "4. ML modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fChb-R-Fu9I"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 58875,
     "status": "ok",
     "timestamp": 1699962423477,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "ioLRjA94FiyG",
    "outputId": "3eff86ef-24d4-460a-80cb-e4afc2544e98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "# Linear algebra\n",
    "import numpy as np\n",
    "# EDA\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# NLTK libraries\n",
    "import nltk\n",
    "nltk.download('all')    # After running all, comment out this line to stop redownloading nltk every time\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# Stemmer & Lemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Wordcloud\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "# Tokenizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# RE\n",
    "import re,string,unicodedata\n",
    "# Bag of Words\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "# Feature Extraction\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# ML models\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Web Scraping tool\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz7-hE3dQu4x"
   },
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PeScqdmrPKpV"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/IMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the dataframe display size\n",
    "pd.options.display.max_colwidth = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPZ-wdnUS4C8"
   },
   "source": [
    "### Inspecting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1699962432172,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "5lrwVLJCQ4Bc",
    "outputId": "0dd2f00f-b259-419c-b48f-db389ee5278f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fas...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air condition...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a v...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                          review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are ...   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fas...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air condition...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents a...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a v...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1699962434737,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "xceQYBrzSLKS",
    "outputId": "18f1f0c1-be5b-4f47-914d-57395aa0a1d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs7l-jq4TKmE"
   },
   "source": [
    "## 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1699962438647,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "zRUYusYlTIYc",
    "outputId": "7a8a0c70-7a46-4968-df0f-f1cbcdc06e18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stim...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                               review  \\\n",
       "count                                                                                                           50000   \n",
       "unique                                                                                                          49582   \n",
       "top     Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stim...   \n",
       "freq                                                                                                                5   \n",
       "\n",
       "       sentiment  \n",
       "count      50000  \n",
       "unique         2  \n",
       "top     positive  \n",
       "freq       25000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1699962448490,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "HdEh23MkTQ_Z",
    "outputId": "2c289733-8d5c-44b6-c309-f6c9f8d6a8b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting sentiments\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En8UoW5RUz1x"
   },
   "source": [
    "## 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1699962460023,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "ZzAPFuhF7ibM",
    "outputId": "e0c6ee9d-7257-4f10-e437-66855f8ad7dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fas...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air condition...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a v...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job. It wasn't as creative or original as the first, but who wa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic directing, the annoying porn groove soundtrack that ran contin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary schools by nuns, taught by Jesuit priests in high school &amp; ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previous comment and side with Maltin on this one. This is a second...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              review  \\\n",
       "0      One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are ...   \n",
       "1      A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fas...   \n",
       "2      I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air condition...   \n",
       "3      Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents a...   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a v...   \n",
       "...                                                                                                              ...   \n",
       "49995  I thought this movie did a down right good job. It wasn't as creative or original as the first, but who wa...   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic directing, the annoying porn groove soundtrack that ran contin...   \n",
       "49997  I am a Catholic taught in parochial elementary schools by nuns, taught by Jesuit priests in high school & ...   \n",
       "49998  I'm going to have to disagree with the previous comment and side with Maltin on this one. This is a second...   \n",
       "49999  No one expects the Star Trek movies to be high art, but the fans do expect a movie that is as good as some...   \n",
       "\n",
       "      sentiment  \n",
       "0      positive  \n",
       "1      positive  \n",
       "2      positive  \n",
       "3      negative  \n",
       "4      positive  \n",
       "...         ...  \n",
       "49995  positive  \n",
       "49996  negative  \n",
       "49997  negative  \n",
       "49998  negative  \n",
       "49999  negative  \n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick visualisation of dataset (First 5 + Last 5 rows)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the 'data' dataframe to work off from\n",
    "data_IMDB = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "XY1V4mz_Tf1m"
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# Setting English stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1699933674001,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "gGxBnkil6u4N",
    "outputId": "45f7b6e6-a83e-4637-d973-50f96d165da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example text to tokenize\n",
    "text = \"This is an example sentence for tokenization.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YcKKbTZXkdX"
   },
   "source": [
    "### Text Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "RZaeg2-nV7aJ"
   },
   "outputs": [],
   "source": [
    "# Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# Removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern,'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1699934775369,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "Sptmkrsl7v9A",
    "outputId": "d865fb27-732e-4140-a8f0-c0090b64ecbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is bold and italic.\n",
      "This is  with  sets of .\n",
      "Hello world 123\n"
     ]
    }
   ],
   "source": [
    "# Example of our 3 defined functions\n",
    "html_text = \"<p>This is <b>bold</b> and <i>italic</i>.</p>\"\n",
    "plain_text = strip_html(html_text)\n",
    "print(plain_text)\n",
    "\n",
    "\n",
    "input_text = \"This is [some text] with [multiple] sets of [square brackets].\"\n",
    "result = remove_between_square_brackets(input_text)\n",
    "print(result)\n",
    "\n",
    "\n",
    "input_text = \"Hello, @world! 123\"\n",
    "result2 = remove_special_characters(input_text)\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "NobuMs_1YHKo"
   },
   "outputs": [],
   "source": [
    "# Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_special_characters(text)\n",
    "    return text\n",
    "\n",
    "# Apply function on review column\n",
    "data_IMDB['review'] = data_IMDB['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        inmate face focus high Muslims meThe surreal State drug front agreement away turned wordIt would regard aw...\n",
       "1        only mural solid very come he Orton filming oldtimeBBC with written really performed are wonderful decorat...\n",
       "2        interesting decade tone Allen Prada still way plot loveThis character conditioned with year hot dialogue a...\n",
       "3        like zombie shot totally parent fighting 10 expected arguing real with are for make dialog some of become ...\n",
       "4        city money only soul come transfer previous wish portrait way encounterThe picture character luxurious wit...\n",
       "                                                             ...                                                      \n",
       "49995    like Im money soul very come expecting enjoy thats havent recommend wasnt office for classic creative alwa...\n",
       "49996    only could plot had porn script dialogue for of copy redeemed it spark and annoying figure time overacted ...\n",
       "49997    high love church very attempt still loss would way real supposed tragedy act with intellectual former are ...\n",
       "49998    like Andrew complacency Im hillside bowling second previous pin Coburn modern rate would real rape Mitchum...\n",
       "49999    Fan nine high only art plot would way character had far channel with youll episode for some of including i...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_IMDB['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "st6343ngY31g"
   },
   "outputs": [],
   "source": [
    "# Lemmatizing the text\n",
    "def simple_lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join({lemmatizer.lemmatize(word) for word in text.split()})\n",
    "    return text\n",
    "\n",
    "# Apply function on review column\n",
    "data_IMDB['review'] = data_IMDB['review'].apply(simple_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1699936065808,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "U1m6gf1RC1H8",
    "outputId": "9151db4b-177f-4d00-ca63-363ecc586de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fast. are running eating. am and I car\n"
     ]
    }
   ],
   "source": [
    "# Example of lemmatizing text using sample sentence\n",
    "input_text = \"I am running and eating. The cars are running fast.\"\n",
    "lemmatized_text = simple_lemmatize(input_text)\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2J-v8LfHwZp"
   },
   "source": [
    "### Initial Data-Preprocessing Verdict\n",
    "Lemmatizer is not as accurate as we want it to be\n",
    "- Sentences are garbled and in a mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-Of-Speech (POS) tagging\n",
    "Implement Part-Of-Speech (POS) tagging to improve accuracy.\n",
    "\n",
    "This helps the algorithm understand the grammatical structure and meaning of a text.\n",
    "\n",
    "For example, consider the sentence: \"The cat is sleeping on the mat.\"\n",
    "\n",
    "POS tagging would assign the following tags:\n",
    "- \"The\" - determiner (DT)\n",
    "- \"cat\" - noun (NN)\n",
    "- \"is\" - verb (VBZ)\n",
    "- \"sleeping\" - verb (VBG)\n",
    "- \"on\" - preposition (IN)\n",
    "- \"the\" - determiner (DT)\n",
    "- \"mat\" - noun (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1699963521269,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "64tl9OmVGP7y",
    "outputId": "d51da1c5-ccb7-467f-f47b-28aeb6a6a8eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lijaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to process the pos_tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    else:\n",
    "        return 'n'  # Default to noun for unknown or uncategorized words\n",
    "\n",
    "# Redefining the lemmatizer function\n",
    "def simple_lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1699963567194,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "jiiP5IcNHDkv",
    "outputId": "7dac2cc6-f252-4f66-e78b-1a4389f2a1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I be run and eat . The car be run fast .\n"
     ]
    }
   ],
   "source": [
    "# Test if the new function works as intended\n",
    "input_text = \"I am running and eating. The cars are running fast.\"\n",
    "lemmatized_text = simple_lemmatize(input_text)\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "9_35d4QVOOhG"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Can we apply the new function on review column?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_IMDB[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_IMDB\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple_lemmatize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# We are not going to do it this way; computationally expensive, time consuming\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[44], line 25\u001b[0m, in \u001b[0;36msimple_lemmatize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     23\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m     24\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[1;32m---> 25\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m lemmatized_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, pos\u001b[38;5;241m=\u001b[39mget_wordnet_pos(tag)) \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m pos_tags]\n\u001b[0;32m     27\u001b[0m lemmatized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_tokens)\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:169\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    168\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:126\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:201\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[0;32m    200\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[1;32m--> 201\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[0;32m    204\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:86\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[1;34m(self, features, return_conf)\u001b[0m\n\u001b[0;32m     83\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# compute the confidence\u001b[39;00m\n\u001b[0;32m     88\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_softmax(scores)) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lijaa\\Desktop\\MY-DAC-Curriculum\\.venv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:86\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict.<locals>.<lambda>\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m     83\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# compute the confidence\u001b[39;00m\n\u001b[0;32m     88\u001b[0m conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_softmax(scores)) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Can we apply the new function on review column?\n",
    "data_IMDB['review'] = data_IMDB['review'].apply(simple_lemmatize)\n",
    "# We are not going to do it this way; computationally expensive, time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1699963826066,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "eadnMU3TZS73",
    "outputId": "759806e5-89a3-4458-dac9-ba97d4394533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'itself', 'was', 'only', 'very', 'further', \"don't\", 'once', 'don', 'any', 'whom', 'he', 'had', 'm', 'themselves', 'with', \"won't\", 'while', 'own', 'doing', 'are', 'for', \"you've\", 'couldn', 'hasn', \"wouldn't\", \"hasn't\", 'same', 'again', 'under', 'each', 'some', 'of', 'up', \"haven't\", \"doesn't\", 'herself', 'it', 'above', 'do', 'and', 'why', 'these', 'were', 'ours', \"mustn't\", 'below', 'wasn', 'both', 'about', 'am', 'all', 'ourselves', \"weren't\", 'being', 'after', 'we', 'you', 'having', 'will', \"you'll\", 'theirs', 'those', 'o', 'if', 'should', 'weren', 'him', 'mustn', 'who', 'ma', \"isn't\", 'she', 'most', \"it's\", 're', 'haven', 'didn', \"couldn't\", 'or', 'few', 'too', 'yourself', 'until', 'at', 'ain', 'needn', 've', 'does', 'just', 'its', \"needn't\", 'his', 'them', 'now', \"hadn't\", 'y', 'yours', 'through', 'there', 'how', 'the', 'over', 't', 'they', \"she's\", \"you're\", 'is', 'nor', 'her', 'to', \"should've\", 'then', 'mightn', 'from', 'between', 'but', 'did', \"mightn't\", 'myself', 'aren', 'this', 'yourselves', 'down', 'me', 'can', \"you'd\", \"shan't\", \"wasn't\", 'which', 'before', 'here', \"aren't\", 'isn', 'won', 'your', 'in', 'be', 'into', \"didn't\", 'an', 'i', 'than', 'our', 'during', 'have', 'their', 'shouldn', 'my', 'off', 'not', 'hadn', 'as', 'when', 'on', 'against', \"that'll\", 'hers', 'other', \"shouldn't\", 'because', 'shan', 'out', 's', 'no', 'himself', 'that', 'been', 'has', 'a', 'more', 'where', 'such', 'what', 'd', 'by', 'll', 'wouldn', 'so', 'doesn'}\n",
      "This example sentence stopwords .\n"
     ]
    }
   ],
   "source": [
    "# Set stopwords to English\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "# Removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# Example to test out our stopword-removing function\n",
    "input_text = \"This is an example sentence with some stopwords.\"\n",
    "filtered_text = remove_stopwords(input_text, is_lower_case=True)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "4dhovTtzODXE"
   },
   "outputs": [],
   "source": [
    "# Apply function on 'review' column\n",
    "data_IMDB['review'] = data_IMDB['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cli3CZ2cfnk"
   },
   "source": [
    "### Text Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1699964050294,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "pU4Dl2a8cjI7",
    "outputId": "45f5eac5-31b6-49e7-864a-f35c04c90d95"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inmate face focus Muslims high meThe surreal State drug front agreement away turned wordIt would regard aw...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mural solid come Orton filming oldtimeBBC written really performed wonderful decorating technique guard ha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interesting decade tone Allen Prada still way plot loveThis character conditioned year hot dialogue wonder...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like zombie shot totally parent fighting 10 expected arguing real make dialog become drama descent first B...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>city money soul come previous transfer wish portrait way encounterThe picture character luxurious directio...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>like Im money soul come expecting enjoy thats havent recommend wasnt office classic creative always isnt b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>could plot porn script dialogue copy redeemed spark annoying figure time overacted continually girl direct...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>high love church attempt still loss would way real supposed tragedy act intellectual former killed Marys f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>like Andrew complacency Im hillside bowling second previous pin Coburn modern rate would real rape Mitchum...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Fan nine high art plot would way character far channel youll episode including cable worth movie need scen...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              review  \\\n",
       "0      inmate face focus Muslims high meThe surreal State drug front agreement away turned wordIt would regard aw...   \n",
       "1      mural solid come Orton filming oldtimeBBC written really performed wonderful decorating technique guard ha...   \n",
       "2      interesting decade tone Allen Prada still way plot loveThis character conditioned year hot dialogue wonder...   \n",
       "3      like zombie shot totally parent fighting 10 expected arguing real make dialog become drama descent first B...   \n",
       "4      city money soul come previous transfer wish portrait way encounterThe picture character luxurious directio...   \n",
       "...                                                                                                              ...   \n",
       "49995  like Im money soul come expecting enjoy thats havent recommend wasnt office classic creative always isnt b...   \n",
       "49996  could plot porn script dialogue copy redeemed spark annoying figure time overacted continually girl direct...   \n",
       "49997  high love church attempt still loss would way real supposed tragedy act intellectual former killed Marys f...   \n",
       "49998  like Andrew complacency Im hillside bowling second previous pin Coburn modern rate would real rape Mitchum...   \n",
       "49999  Fan nine high art plot would way character far channel youll episode including cable worth movie need scen...   \n",
       "\n",
       "      sentiment  \n",
       "0      positive  \n",
       "1      positive  \n",
       "2      positive  \n",
       "3      negative  \n",
       "4      positive  \n",
       "...         ...  \n",
       "49995  positive  \n",
       "49996  negative  \n",
       "49997  negative  \n",
       "49998  negative  \n",
       "49999  negative  \n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a variable for the normalized dataframe and add the data_IMDB\n",
    "norm_data_IMDB = data_IMDB\n",
    "norm_data_IMDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1699964095794,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "72xBxKCGi267",
    "outputId": "90504563-38d6-4432-ee3a-80879f4ab236"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data_IMDB.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MomAYMLdleo-"
   },
   "source": [
    "## 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLy9vhfxg4kf"
   },
   "source": [
    "### Method 1: Bag of Words\n",
    "\n",
    "The \"Bag of Words\" (BoW) model is a common and simple representation used in natural language processing (NLP) and information retrieval.\n",
    "\n",
    "It's a way of converting text data into numerical vectors that can be used by machine learning algorithms\n",
    "\n",
    "TLDR: Based on the raw word counts and is suitable when you want to capture the frequency of words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1699964220494,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "mmNYt8w9P5io",
    "outputId": "b73f9c6d-72bc-4204-dc4c-8c9915e28c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  and this  and this is  document  document is  document is the  first  \\\n",
      "0    0         0            0         1            0                0      1   \n",
      "1    0         0            0         2            1                1      0   \n",
      "2    1         1            1         0            0                0      0   \n",
      "3    0         0            0         1            0                0      1   \n",
      "\n",
      "   first document  is  is the  ...  the third one  third  third one  this  \\\n",
      "0               1   1       1  ...              0      0          0     1   \n",
      "1               0   1       1  ...              0      0          0     1   \n",
      "2               0   1       1  ...              1      1          1     1   \n",
      "3               1   1       0  ...              0      0          0     1   \n",
      "\n",
      "   this document  this document is  this is  this is the  this the  \\\n",
      "0              0                 0        1            1         0   \n",
      "1              1                 1        0            0         0   \n",
      "2              0                 0        1            1         0   \n",
      "3              0                 0        0            0         1   \n",
      "\n",
      "   this the first  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               1  \n",
      "\n",
      "[4 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Example documents in list form\n",
    "documents = [\"This is the first document.\",\n",
    "              \"This document is the second document.\",\n",
    "              \"And this is the third one.\",\n",
    "              \"Is this the first document?\"]\n",
    "\n",
    "# Create an instance of the CountVectorizer class, where ngram ranges from 1 word to 3 words\n",
    "# Unigram = singular word / Bigram = 2 words\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "\n",
    "# Fit and transform the documents into a Bag of Words representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (words) that correspond to the columns in the Bag of Words matrix\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the Bag of Words matrix to an array for better visualization\n",
    "X_array = X.toarray()\n",
    "\n",
    "# DataFrame for better visualization\n",
    "df_bow = pd.DataFrame(X_array, columns=feature_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "43qMqyzyvPN6"
   },
   "outputs": [],
   "source": [
    "# # Fitting our data into the CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(1,3)).fit(norm_data_IMDB['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "eY7GWKlPwZDT"
   },
   "outputs": [],
   "source": [
    "# Getting the feature names from the vectorised features\n",
    "feature_names = vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1699964447722,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "92o5NyXJUfl8",
    "outputId": "d954069d-8bb9-4adf-f0dd-afc06ea2e7a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '00 decorated', '00 decorated mansion', ...,\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz',\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz last',\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz last hawke'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1699964471392,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "rdbMi9f7acuY",
    "outputId": "931958b9-acdb-4d07-a739-ed1bac5ded8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    inmate face focus Muslims high meThe surreal State drug front agreement away turned wordIt would regard aw...\n",
       "1    mural solid come Orton filming oldtimeBBC written really performed wonderful decorating technique guard ha...\n",
       "2    interesting decade tone Allen Prada still way plot loveThis character conditioned year hot dialogue wonder...\n",
       "3    like zombie shot totally parent fighting 10 expected arguing real make dialog become drama descent first B...\n",
       "4    city money soul come previous transfer wish portrait way encounterThe picture character luxurious directio...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data_IMDB['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "2ixbcLk4wtIh"
   },
   "outputs": [],
   "source": [
    "# Extract the feature 'review'\n",
    "X_cv = norm_data_IMDB['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1699964516939,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "Ilqr94ZyRgKQ",
    "outputId": "53fc2030-6c03-4007-bced-0c4aeab18732"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "AJQgcjzuwzfL"
   },
   "outputs": [],
   "source": [
    "# Extract the target 'sentiment'\n",
    "Y_cv = norm_data_IMDB['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "JXtC_bJFw-nr"
   },
   "outputs": [],
   "source": [
    "# Transforming the feature 'review' data\n",
    "X_cv = vect.transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1699964601143,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "mITBHBOHZw12",
    "outputId": "10369e45-84ae-40cc-aadd-a443136346a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 6425079)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFM3ueoMk4Qw"
   },
   "source": [
    "#### Method 2: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGQVk1RwDwdG"
   },
   "source": [
    "Term Frequency (TF):\n",
    "\n",
    "The TF component measures how often a term appears in a document. It's a raw count of the number of times the term occurs within the document.\n",
    "TF is calculated for each term within each document.\n",
    "\n",
    "Inverse Document Frequency (IDF):\n",
    "\n",
    "The IDF component evaluates how important a term is across the entire corpus(enitre body of text). It's a measure of how unique or rare a term is.\n",
    "Terms that appear frequently in many documents have a lower IDF, while terms that appear in a smaller subset of documents have a higher IDF.\n",
    "\n",
    "TLDR: Considers not only the frequency of words but also their importance across the entire set of documents. It helps in emphasizing words that are more discriminative and less common across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "by0RVet1xy29"
   },
   "outputs": [],
   "source": [
    "# Create TFIDF vectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "RlpSc6cDx2tO"
   },
   "outputs": [],
   "source": [
    "# Apply TFIDF transformer to 'review' column\n",
    "X_tf = tfidf.fit_transform(norm_data_IMDB['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1699964747792,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "yUjgr06JyDlh",
    "outputId": "66ba1ef7-c99c-46ea-86fa-1184ee5dc7df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000000000001', ..., 'zzzzzzzzzzzzz',\n",
       "       'zzzzzzzzzzzzzzzzzz', 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1699964764120,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "_e7PzJczyNES",
    "outputId": "baa3f39c-452b-4ba8-e9e5-409c59b8be61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 215971)\n"
     ]
    }
   ],
   "source": [
    "print(X_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "cMYTtn3yypB-"
   },
   "outputs": [],
   "source": [
    "# Extract the target 'sentiment'\n",
    "Y_tf = norm_data_IMDB['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkYW8Q6Ak_tf"
   },
   "source": [
    "### Labelling the 'sentiment' text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1699964891484,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "gF-yWm6nlCOf",
    "outputId": "8c211f08-4e87-44df-ca48-87daf2adda3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Setting up the LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# Transforming and Labelling the 'sentiment' data\n",
    "sentiment_data = lb.fit_transform(data_IMDB['sentiment'])\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGGWtvOMliTf"
   },
   "source": [
    "## 4. ML Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOhdJJDZ9cJc"
   },
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MJbEOPoVm5U"
   },
   "source": [
    "#### Logistic Regression - Bags of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD5S3amNzHRa"
   },
   "outputs": [],
   "source": [
    "# Setting up the LogisticRegression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Au45psF3D2Ja"
   },
   "outputs": [],
   "source": [
    "# # Split arrays/matrices into random train and test subsets. In this case, 80:20 for Train:Test ratio\n",
    "# x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(X_cv, Y_cv, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198736,
     "status": "ok",
     "timestamp": 1699965129056,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "0moO6dI1VlCk",
    "outputId": "c1accca3-14c0-406a-dbd2-12efe5a5fa74"
   },
   "outputs": [],
   "source": [
    "# Fitting the lr model for Bag of Words\n",
    "\n",
    "\n",
    "\n",
    "# Predicting the lr model for Bag of Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZf1teCgVxJM"
   },
   "source": [
    "#### Logistic Regression - TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVfCailSD7fX"
   },
   "outputs": [],
   "source": [
    "# # Split arrays/matrices into random train and test subsets. In this case, 80:20 for Train:Test ratio\n",
    "# x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(X_tf, Y_tf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15502,
     "status": "ok",
     "timestamp": 1699965328302,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "O8Et6na74Dq6",
    "outputId": "bfb958cc-8e50-46c0-fed3-1333507f89b3"
   },
   "outputs": [],
   "source": [
    "# # Fitting the lr model for TFIDF features\n",
    "# lr_tfidf = lr.fit(x_train_tf, y_train_tf)\n",
    "# print(lr_tfidf)\n",
    "\n",
    "# # Predicting the lr model for TFIDF features\n",
    "# lr_tfidf_predict = lr.predict(x_test_tf)\n",
    "# print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I92eyxS364-O"
   },
   "source": [
    "#### Logistic Regression - Accuracy Scores & Classification Report for both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1699965332552,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "VWI9pVdM6zg6",
    "outputId": "4ad8a685-5aed-433e-e82c-62ebcd4c407f"
   },
   "outputs": [],
   "source": [
    "# Accuracy score for Bag of Words\n",
    "\n",
    "\n",
    "\n",
    "# Accuracy score for TFIDF features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1699934183667,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "LqVeOEMU7o8B",
    "outputId": "3d6415cc-7403-4d95-c756-3c4519abf093"
   },
   "outputs": [],
   "source": [
    "# Classification report for Bag of Words\n",
    "\n",
    "\n",
    "\n",
    "# Classification report for TFIDF features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKvSqPOi8RN-"
   },
   "source": [
    "#### Logistic Regression - Confusion Matrix for both Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcBlCbcgWubg"
   },
   "source": [
    "##### For Bags of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1699965658821,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "CHnsIfNE8N6O",
    "outputId": "6ab01464-012e-4635-9b8b-61d07ba3c878"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqTratyrWxVG"
   },
   "source": [
    "##### For TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1699965786848,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "AYKRQGjl8atp",
    "outputId": "2cdd78bf-abd3-4011-c3fa-ba2ca868fa9e"
   },
   "outputs": [],
   "source": [
    "# cm_tf = confusion_matrix(y_test_tf, lr_tfidf_predict, labels=lr.classes_)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm_tf, display_labels=lr.classes_)\n",
    "# disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAELwplY9lRO"
   },
   "source": [
    "### Model 2: Multinomial Naive Bayes (MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qBO_5xuWFOn"
   },
   "source": [
    "#### MNB - Bags of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1699965969576,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "q5wkjSj99sDu",
    "outputId": "eb13a93f-5fb8-4a99-d935-1dea52e8a057"
   },
   "outputs": [],
   "source": [
    "# Training the Multinomial Naive Bayes model\n",
    "\n",
    "\n",
    "\n",
    "# Fitting the MNB for Bag of Words\n",
    "\n",
    "\n",
    "\n",
    "# Predicting the model for Bag of Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsZAdvhoWKr3"
   },
   "source": [
    "#### MNB - TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1699965883012,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "ovMwFrp991vq",
    "outputId": "5d54ebb6-e594-446b-c65d-519c1c8591f5"
   },
   "outputs": [],
   "source": [
    "# # Fitting the MNB for TFIDF features\n",
    "# mnb_tfidf = mnb.fit(x_train_tf, y_train_tf)\n",
    "# print(mnb_tfidf)\n",
    "\n",
    "# # Predicting the MNB model for TFIDF features\n",
    "# mnb_tfidf_predict = mnb.predict(x_test_tf)\n",
    "# print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hj3qyuKWOoK"
   },
   "source": [
    "#### MNB - Accuracy Scores for both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1699966061174,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "FxVvLhgd9_Z5",
    "outputId": "5f02624d-8c65-49ac-d8bc-649fbf158444"
   },
   "outputs": [],
   "source": [
    "# # Accuracy score for Bag of Words\n",
    "# mnb_bow_score = accuracy_score(y_test_cv, mnb_bow_predict)\n",
    "# print('mnb_bow_score : {:.2f}%'.format(mnb_bow_score*100))\n",
    "\n",
    "# # Accuracy score for TFIDF features\n",
    "# mnb_tfidf_score = accuracy_score(y_test_tf, mnb_tfidf_predict)\n",
    "# print('mnb_tfidf_score : {:.2f}%'.format(mnb_tfidf_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDWkyGJuWSOE"
   },
   "source": [
    "#### MNB - Confusion Matrix for both Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KQkYOrOWYjT"
   },
   "source": [
    "##### For Bags of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 1641,
     "status": "ok",
     "timestamp": 1699966370338,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "fRqQVgDC-mX5",
    "outputId": "e606d671-f3d0-4a01-f2b0-856300204427"
   },
   "outputs": [],
   "source": [
    "# cm_cv_mnb = confusion_matrix(y_test_cv, mnb_bow_predict, labels=mnb.classes_)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix = cm_cv_mnb, display_labels = mnb.classes_)\n",
    "# disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PastJOQ7WfV1"
   },
   "source": [
    "##### For TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 1672,
     "status": "ok",
     "timestamp": 1699966388056,
     "user": {
      "displayName": "Parth Uday",
      "userId": "17936957073811628306"
     },
     "user_tz": -480
    },
    "id": "9M0Yo9Vm_AXk",
    "outputId": "3647239a-87e7-4814-a80f-8e6217cb4b30"
   },
   "outputs": [],
   "source": [
    "# cm_tf_mnb = confusion_matrix(y_test_tf, mnb_tfidf_predict, labels=mnb.classes_)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix = cm_tf_mnb, display_labels = mnb.classes_)\n",
    "# disp.plot()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XLy9vhfxg4kf",
    "kkYW8Q6Ak_tf",
    "I92eyxS364-O",
    "KcBlCbcgWubg",
    "vqTratyrWxVG"
   ],
   "provenance": [
    {
     "file_id": "1zcrp3gK6repODBPmrSq7T1X1HmQKKlvm",
     "timestamp": 1700239150862
    },
    {
     "file_id": "1KXITORjL7oLNg0MqRKuMoLm3iN7UfrcL",
     "timestamp": 1699955804539
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
